{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Twitter, @Asmodaie asked me this question: \"If team A has a win rate of 20% and team B a win rate of 60% what are the odds of A winning from B?\"\n",
    "\n",
    "This is one of those probabiltiy questions that can't be answered simply by following the usual rules of probability theory. We're going to need to make some extra assumptions that we can try to make as reasonable as possible. I also don't pretend to know the best answer. I'm just going to give three approaches that are all somewhat heuristic and could easily be argued with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 (Simulation)\n",
    "========\n",
    "\n",
    "If you know nothing about a pair of teams, you'd estimate that each team has a 0.5 chance of beating the other (assuming no draws.) So let's try to make that *Assumption 1* somehow.\n",
    "\n",
    "What does it mean for A to have a win rate of 20%? Suppose we have infinitely many teams, of which A and B are just two. If we arranged a tournament and got all the teams to play against each other then we'd expect A to win 20% of these games. By \"expect\" here we mean something like if we repeated the entire tournament infinitely often, A would win exactly 20%. Or, given that a tournament has infinitely many teams, we may as well go with precisely 20% of the games in one tournament. Let's make that *Assumption 2*.\n",
    "\n",
    "I'd like to first approach this problem with a Monte Carlo simulation. \"Infinitely often\" can be a challenge for computers. So instead, I'll go with a finite number of teams, in this case I'll start with just 6. (You'll see why I picked 6 later.)\n",
    "\n",
    "So this is the simulation I want to perform: if we have N teams, there are N-2 games of A vs everyone else except B, there are N-2 games of B vs everyone else except A, and 1 game between A and B. We'll assume each game has a 50/50 chance of being a win for either team (Assumption 1). But we're going to condition on the outcome that A wins p (=0.2) of the games and B wins q(=0.6) (Assumption 2). Now you can see why I picked 6, I wanted p(N-1) and q(N-1) to be integers.\n",
    "\n",
    "Here's a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47172 55036 0.8571117086997602\n"
     ]
    }
   ],
   "source": [
    "N = 6\n",
    "p = 0.6\n",
    "q = 0.2\n",
    "u = 0\n",
    "v = 0\n",
    "batch = 1000\n",
    "for i in xrange(1000):\n",
    "    # Simulate 1,000,000 games in batches of 1000\n",
    "    ax = scipy.stats.bernoulli.rvs(0.5, size=(batch, N-2))\n",
    "    bx = scipy.stats.bernoulli.rvs(0.5, size=(batch, N-2))\n",
    "    ab = scipy.stats.bernoulli.rvs(0.5, size=batch)\n",
    "    na = numpy.sum(ax, axis=-1)\n",
    "    nb = numpy.sum(bx, axis=-1)\n",
    "    a = na+ab\n",
    "    b = nb+1-ab\n",
    "\n",
    "    # Consider only those which have both of the desired outcome\n",
    "    w = (np.round(a) == np.round(p*(N-1)))*(np.round(b) == np.round(q*(N-1)))\n",
    "    # Sum number of wins for A vs B\n",
    "    u += np.sum(w*ab)\n",
    "    # Count total number of games with proportions p and q\n",
    "    v += np.sum(w)\n",
    "print u, v, u/v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now try cranking up N. But there's a small problem. Despite simulating 1,000,000 games, for large N the number of games satisfying our condition is small. For example, in 20 games it's relatively rare to get exactly 12 wins for A and exactly 4 wins for B. The method works but we're wasting a bit of CPU time to get inaccurate estimates.\n",
    "\n",
    "A nice trick here is to use importance sampling. Start with\n",
    "\n",
    "$\\mathbb{E}[X] = \\sum_i p(X_i)X_i$\n",
    "\n",
    "where we're summing over all possible outcomes. We're approximating this in our simulation by using $p(X_i)$ as the probability we use to generate samples, and summing the terms $X_i$ for each sample.\n",
    "\n",
    "Suppose we have another probability distribution $q$. Then we have\n",
    "\n",
    "$\\mathbb{E}[X] = \\sum_i q(X_i)\\frac{p(X_i)}{q(X_i)}X_i$\n",
    "\n",
    "We can interpret this as saying that we can use distribution $q$ to generate our samples as long as we scale each $X_i$ in the sum by a factor of $\\frac{p(X_i)}{q(X_i)}$. The reason we'd like to do this is that despite wanting to compute probabilities assuming even odds for each game, we can choose probabilities that make our desired outcomes more likely as long as we scale their contribution appropriately.\n",
    "\n",
    "In our case we need to scale by $\\frac{0.5}{p}$ each time we generated a win for A, $\\frac{0.5}{1-p}$ each time we generated a loss for A, $\\frac{0.5}{q}$ for each win for B and $\\frac{0.5}{1-q}$ for each loss for B. We'll use 50/50 for A vs B though. This will allow us to go up to 21 teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8570287185405475\n"
     ]
    }
   ],
   "source": [
    "def run(p, q):\n",
    "    m = 21\n",
    "    u = 0\n",
    "    v = 0\n",
    "    batch = 1000\n",
    "    for i in xrange(1000):\n",
    "        # Replaced bernoulli with binom assuming it's a \n",
    "        # more efficient way of doing same thing\n",
    "        na = scipy.stats.binom.rvs(m-2, p, size=batch)\n",
    "        nb = scipy.stats.binom.rvs(m-2, q, size=batch)\n",
    "        ab = scipy.stats.bernoulli.rvs(0.5, size=batch)\n",
    "        a = na+ab\n",
    "        b = nb+1-ab\n",
    "\n",
    "        weight = (0.5/p)**na*(0.5/(1-p))**(m-2-na)*(0.5/q)**nb*(0.5/(1-q))**(m-2-nb)\n",
    "        w = weight*(a == p*(m-1))*(b == q*(m-1))\n",
    "        u += np.sum(w*ab)\n",
    "        v += np.sum(w)\n",
    "    return u/v\n",
    "\n",
    "print run(0.6, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tabulate these results for various p and q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "m1 = [[run(b, a)\n",
    "       for a in p]\n",
    "      for b in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|p\\q|0.2|0.4|0.6|0.8|\n",
       "|-|-|-|-|-|\n",
       "|**0.2**|0.503|0.272|0.143|0.060|\n",
       "|**0.4**|0.727|0.501|0.309|0.142|\n",
       "|**0.6**|0.855|0.690|0.504|0.275|\n",
       "|**0.8**|0.942|0.857|0.726|0.497|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def tabulate(p, m):\n",
    "    s = \"\"\n",
    "    s += \"|p\\\\q|\" + \"|\".join([str(p[j]) for j in range(len(p))]) + \"|\" + \"\\n\"\n",
    "    s += \"|\" + (len(m)+1)*\"-|\" + \"\\n\"\n",
    "    for i in range(len(m)):\n",
    "        s += (\"|**%.1f**|\" % p[i]) + \"|\".join([\"%.3f\" % m[i][j]\n",
    "                                              for j in range(len(m))]) + \"|\" + \"\\n\"\n",
    "\n",
    "    display(Markdown(s))\n",
    "    \n",
    "tabulate(p, m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle we didn't have to run a simulation. It's not hard to use the binomial theorem to compute the probabilities for any $N$ and then take the limit. But I liked the idea of running a simple simulation that does nothing more than coin tosses so there's no sleight of hand. (And note that importance sampling is just a speed-up. You *could* use just the fair coin version.)\n",
    "\n",
    "Method 2 (Maximum Entropy)\n",
    "--------\n",
    "\n",
    "We can try a maximum entropy approach.\n",
    "\n",
    "Suppose we have N teams. Let A be team 0 and B be team 1. Let $a_i$ be the probability of $A$ beating team $i$ for $i=2\\ldots N-1$. Similarly let $b_i$ be the probability of $B$ beating team $i$ for $i=2\\ldots N-1$. And let $c$ be the probability of A beating B. We want to maximise\n",
    "\n",
    "$c\\log c+(1-c)\\log(1-c)+\\sum_{i=2}^{N-1}\\left(a_i\\log a_i+(1-a_i)\\log (1-a_i)+b_i\\log b_i+(1-b_i)\\log (1-b_i)\\right)$\n",
    "\n",
    "subject to the constraints\n",
    "\n",
    "$c+\\sum_{i=2}^{N-1} a_i = p(N-1)$ and $1-c+\\sum_{i=2}^{N-1} b_i = q(N-1)$.\n",
    "\n",
    "Introducing Lagrange multipliers define\n",
    "\n",
    "$L(c,a_i,b_i,\\lambda,\\mu) = c\\log c+(1-c)\\log(1-c)+\\sum_{i=2}^{N-1}\\left(a_i\\log a_i+(1-a_i)\\log (1-a_i)+b_i\\log b_i+(1-b_i)\\log (1-b_i)\\right)+\\lambda(c+\\sum_{i=2}^{N-1} a_i-p(N-1))+\\mu(1-c+\\sum_{i=2}^{N-1} b_i-q(N-1))$\n",
    "\n",
    "Differentiating with respect to $c$, $a_i$ and $b_i$, setting the results to zero, and rearranging, we get:\n",
    "\n",
    "$c=\\frac{\\exp(\\mu-\\lambda)}{1+\\exp{\\mu-\\lambda}}$,\n",
    "\n",
    "$a_i = \\frac{\\exp(-\\lambda)}{1+\\exp{-\\lambda}}$\n",
    "\n",
    "and \n",
    "\n",
    "$b_i = \\frac{\\exp(-\\mu)}{1+\\exp{-\\mu}}$.\n",
    "\n",
    "In the limit as N becomes large then the $a_i$ must tend to $p$ and the $b_i$ must tend to $q$. So we can just plug in those values, solve for $\\lambda$ and $\\mu$, and we get\n",
    "\n",
    "$c=\\frac{p-pq}{p+q-2pq}$. Let's tabulate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|p\\q|0.2|0.4|0.6|0.8|\n",
       "|-|-|-|-|-|\n",
       "|**0.2**|0.500|0.273|0.143|0.059|\n",
       "|**0.4**|0.727|0.500|0.308|0.143|\n",
       "|**0.6**|0.857|0.692|0.500|0.273|\n",
       "|**0.8**|0.941|0.857|0.727|0.500|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(p, q):\n",
    "    return (p-p*q)/(p+q-2*p*q)\n",
    "\n",
    "m2 = [[f(b, a)\n",
    "       for a in p]\n",
    "      for b in p]\n",
    "\n",
    "tabulate(p, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you're at least mildly surprised that this is almost identical to the previous result.\n",
    "\n",
    "Now for one last approach.\n",
    "\n",
    "Method 3 (Bradley-Terry)\n",
    "-------\n",
    "\n",
    "Let's use the really simple model of assuming each team $i$ has a strength given by a positive real number $s_i$. The probability of $i$ beating $j$ is assumed to be simply\n",
    "\n",
    "$\\frac{s_i}{s_i+s_j}$\n",
    "\n",
    "This is known as the [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley–Terry_model) model though it goes back to [Zermelo](https://link.springer.com/article/10.1007%2FBF01180541) in the 20s. It sort of expresses the notion that your strength is the number of opportunities you bring to the game and the formula is the probability that the first opportunity to win is yours.\n",
    "\n",
    "So team A has strength $s_A$, team B has strength $S_B$, and in the absence of any information about the other teams let's just assign them all an arbitrary strength, say $s_i=1$.\n",
    "\n",
    "We want $c=\\frac{s_A}{s_A+s_b}$ given that $\\frac{s_A}{s_A+1}=p$ and $\\frac{s_B}{s_B+1}=q$.\n",
    "\n",
    "Solving for $c$ , we get $c=\\frac{p-pq}{p+q-2pq}$.\n",
    "\n",
    "So we have 3 methods, all seemingy giving the same results.\n",
    "\n",
    "**Why does Method 2 equal Method 3?**\n",
    "\n",
    "It seems that the Bradley-Terry model, despite looking like the first thing you might make up, is actually an entropy maximising model, as described in the last paragraph at Wikipedia.\n",
    "\n",
    "**Why does Method 1 approximately equal Method 2?**\n",
    "\n",
    "Entropy maximisation methods can be reinterpreted as conditional probabilities in an appropriate limit. A form of this is described in [Campenhout and Cover](http://www-isl.stanford.edu/~cover/papers/transIT/0483camp.pdf).\n",
    "\n",
    "So, if the only information you have is that A wins $p$ of their games and B wins $q$ then I think $\\frac{p-pq}{p+q-2pq}$ is not a bad guess for the probability of A beating B. But this really is a bare minimum model. If you have any other information it needs to be factored in.\n",
    "\n",
    "One thing I like about Method 1 is that I used only elementary methods. It's nice to see a maximum entropy distribution emerge automatically. I think it can be argued slightly more rigorously that Method 1 is actually an application of [de Finetti's theorem](https://en.wikipedia.org/wiki/De_Finetti%27s_theorem), but that's not something I'm confident to talk about."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
